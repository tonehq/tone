# LLM Metadata Schema
# Supported parameters for each LLM service provider used in get_llm_for_agent()
# Generated by inspecting each class's __init__ and its full inheritance chain.

# ─────────────────────────────────────────────────────────────────────────────
# Common inherited parameters (available to ALL providers):
#   From LLMService:
#     - run_in_parallel: bool (default: true)
#   From FrameProcessor:
#     - name: Optional[str] (default: null)
#     - enable_direct_mode: bool (default: false)
#     - metrics: Optional[FrameProcessorMetrics] (default: null)
# ─────────────────────────────────────────────────────────────────────────────

openai:
  class: OpenAILLMService
  file: pipecat/services/openai/llm.py
  inherits: BaseOpenAILLMService -> LLMService -> AIService -> FrameProcessor
  init_params:
    - name: model
      type: str
      default: "gpt-4.1"
    - name: api_key
      type: str
      default: null
    - name: base_url
      type: str
      default: null
    - name: organization
      type: str
      default: null
    - name: project
      type: str
      default: null
    - name: default_headers
      type: Optional[Mapping[str, str]]
      default: null
    - name: params
      type: Optional[BaseOpenAILLMService.InputParams]
      default: null
    - name: retry_timeout_secs
      type: Optional[float]
      default: 5.0
    - name: retry_on_timeout
      type: Optional[bool]
      default: false
  input_params:
    - name: frequency_penalty
      type: Optional[float]
      default: NOT_GIVEN
      range: "-2.0 to 2.0"
    - name: presence_penalty
      type: Optional[float]
      default: NOT_GIVEN
      range: "-2.0 to 2.0"
    - name: seed
      type: Optional[int]
      default: NOT_GIVEN
      min: 0
    - name: temperature
      type: Optional[float]
      default: NOT_GIVEN
      range: "0.0 to 2.0"
    - name: top_k
      type: Optional[int]
      default: null
      min: 0
      note: "Currently ignored by OpenAI"
    - name: top_p
      type: Optional[float]
      default: NOT_GIVEN
      range: "0.0 to 1.0"
    - name: max_tokens
      type: Optional[int]
      default: NOT_GIVEN
      min: 1
      note: "Deprecated, use max_completion_tokens"
    - name: max_completion_tokens
      type: Optional[int]
      default: NOT_GIVEN
      min: 1
    - name: service_tier
      type: Optional[str]
      default: NOT_GIVEN
    - name: extra
      type: Optional[Dict[str, Any]]
      default: {}

anthropic:
  class: AnthropicLLMService
  file: pipecat/services/anthropic/llm.py
  inherits: LLMService -> AIService -> FrameProcessor
  init_params:
    - name: api_key
      type: str
      required: true
    - name: model
      type: str
      default: "claude-sonnet-4-5-20250929"
    - name: params
      type: Optional[AnthropicLLMService.InputParams]
      default: null
    - name: client
      type: Any
      default: null
    - name: retry_timeout_secs
      type: Optional[float]
      default: 5.0
    - name: retry_on_timeout
      type: Optional[bool]
      default: false
  input_params:
    - name: enable_prompt_caching
      type: Optional[bool]
      default: null
    - name: enable_prompt_caching_beta
      type: Optional[bool]
      default: null
      note: "Deprecated"
    - name: max_tokens
      type: Optional[int]
      default: 4096
      min: 1
    - name: temperature
      type: Optional[float]
      default: NOT_GIVEN
      range: "0.0 to 1.0"
    - name: top_k
      type: Optional[int]
      default: NOT_GIVEN
      min: 0
    - name: top_p
      type: Optional[float]
      default: NOT_GIVEN
      range: "0.0 to 1.0"
    - name: thinking
      type: Optional[ThinkingConfig]
      default: NOT_GIVEN
      note: "ThinkingConfig has fields: type (Literal['enabled','disabled']), budget_tokens (int)"
    - name: extra
      type: Optional[Dict[str, Any]]
      default: {}

groq:
  class: GroqLLMService
  file: pipecat/services/groq/llm.py
  inherits: OpenAILLMService -> BaseOpenAILLMService -> LLMService -> AIService -> FrameProcessor
  init_params:
    - name: api_key
      type: str
      required: true
    - name: base_url
      type: str
      default: "https://api.groq.com/openai/v1"
    - name: model
      type: str
      default: "llama-3.3-70b-versatile"
    - name: params
      type: Optional[BaseOpenAILLMService.InputParams]
      default: null
    - name: organization
      type: str
      default: null
    - name: project
      type: str
      default: null
    - name: default_headers
      type: Optional[Mapping[str, str]]
      default: null
    - name: retry_timeout_secs
      type: Optional[float]
      default: 5.0
    - name: retry_on_timeout
      type: Optional[bool]
      default: false
  input_params: *openai_input_params

openrouter:
  class: OpenRouterLLMService
  file: pipecat/services/openrouter/llm.py
  inherits: OpenAILLMService -> BaseOpenAILLMService -> LLMService -> AIService -> FrameProcessor
  init_params:
    - name: api_key
      type: Optional[str]
      default: null
    - name: model
      type: str
      default: "openai/gpt-4o-2024-11-20"
    - name: base_url
      type: str
      default: "https://openrouter.ai/api/v1"
    - name: params
      type: Optional[BaseOpenAILLMService.InputParams]
      default: null
    - name: organization
      type: str
      default: null
    - name: project
      type: str
      default: null
    - name: default_headers
      type: Optional[Mapping[str, str]]
      default: null
    - name: retry_timeout_secs
      type: Optional[float]
      default: 5.0
    - name: retry_on_timeout
      type: Optional[bool]
      default: false
  input_params: *openai_input_params

aws_bedrock:
  class: AWSBedrockLLMService
  file: pipecat/services/aws/llm.py
  inherits: LLMService -> AIService -> FrameProcessor
  init_params:
    - name: model
      type: str
      required: true
    - name: aws_access_key
      type: Optional[str]
      default: null
    - name: aws_secret_key
      type: Optional[str]
      default: null
    - name: aws_session_token
      type: Optional[str]
      default: null
    - name: aws_region
      type: Optional[str]
      default: null
    - name: params
      type: Optional[AWSBedrockLLMService.InputParams]
      default: null
    - name: client_config
      type: Optional[botocore.config.Config]
      default: null
    - name: retry_timeout_secs
      type: Optional[float]
      default: 5.0
    - name: retry_on_timeout
      type: Optional[bool]
      default: false
  input_params:
    - name: max_tokens
      type: Optional[int]
      default: null
      min: 1
    - name: temperature
      type: Optional[float]
      default: null
      range: "0.0 to 1.0"
    - name: top_p
      type: Optional[float]
      default: null
      range: "0.0 to 1.0"
    - name: stop_sequences
      type: Optional[List[str]]
      default: []
    - name: latency
      type: Optional[str]
      default: null
    - name: additional_model_request_fields
      type: Optional[Dict[str, Any]]
      default: {}

google:
  class: GoogleLLMService
  file: pipecat/services/google/llm.py
  inherits: LLMService -> AIService -> FrameProcessor
  init_params:
    - name: api_key
      type: str
      required: true
    - name: model
      type: str
      default: "gemini-2.5-flash"
    - name: params
      type: Optional[GoogleLLMService.InputParams]
      default: null
    - name: system_instruction
      type: Optional[str]
      default: null
    - name: tools
      type: Optional[List[Dict[str, Any]]]
      default: null
    - name: tool_config
      type: Optional[Dict[str, Any]]
      default: null
    - name: http_options
      type: Optional[HttpOptions]
      default: null
  input_params:
    - name: max_tokens
      type: Optional[int]
      default: 4096
      min: 1
    - name: temperature
      type: Optional[float]
      default: null
      range: "0.0 to 2.0"
    - name: top_k
      type: Optional[int]
      default: null
      min: 0
    - name: top_p
      type: Optional[float]
      default: null
      range: "0.0 to 1.0"
    - name: thinking
      type: Optional[ThinkingConfig]
      default: null
      note: "ThinkingConfig has fields: thinking_budget (Optional[int]), thinking_level (Optional[Literal['low','high','medium','minimal']]), include_thoughts (Optional[bool])"
    - name: extra
      type: Optional[Dict[str, Any]]
      default: {}

ollama:
  class: OLLamaLLMService
  file: pipecat/services/ollama/llm.py
  inherits: OpenAILLMService -> BaseOpenAILLMService -> LLMService -> AIService -> FrameProcessor
  init_params:
    - name: model
      type: str
      default: "llama2"
    - name: base_url
      type: str
      default: "http://localhost:11434/v1"
    - name: params
      type: Optional[BaseOpenAILLMService.InputParams]
      default: null
    - name: organization
      type: str
      default: null
    - name: project
      type: str
      default: null
    - name: default_headers
      type: Optional[Mapping[str, str]]
      default: null
    - name: retry_timeout_secs
      type: Optional[float]
      default: 5.0
    - name: retry_on_timeout
      type: Optional[bool]
      default: false
  note: "api_key is hardcoded to 'ollama' internally"
  input_params: *openai_input_params

# The following providers all use BaseOpenAILLMService with a custom base_url.
# They share the same parameter schema.
base_openai_compatible:
  applies_to:
    - azure
    - cerebras
    - nvidia_nim
    - fireworks
    - together
    - perplexity
    - qwen
    - deepseek
    - mistral
    - sambanova
    - grok
  class: BaseOpenAILLMService
  file: pipecat/services/openai/base_llm.py
  inherits: LLMService -> AIService -> FrameProcessor
  init_params:
    - name: model
      type: str
      required: true
    - name: api_key
      type: str
      default: null
    - name: base_url
      type: str
      default: null
    - name: organization
      type: str
      default: null
    - name: project
      type: str
      default: null
    - name: default_headers
      type: Optional[Mapping[str, str]]
      default: null
    - name: params
      type: Optional[BaseOpenAILLMService.InputParams]
      default: null
    - name: retry_timeout_secs
      type: Optional[float]
      default: 5.0
    - name: retry_on_timeout
      type: Optional[bool]
      default: false
  input_params:
    - name: frequency_penalty
      type: Optional[float]
      default: NOT_GIVEN
      range: "-2.0 to 2.0"
    - name: presence_penalty
      type: Optional[float]
      default: NOT_GIVEN
      range: "-2.0 to 2.0"
    - name: seed
      type: Optional[int]
      default: NOT_GIVEN
      min: 0
    - name: temperature
      type: Optional[float]
      default: NOT_GIVEN
      range: "0.0 to 2.0"
    - name: top_k
      type: Optional[int]
      default: null
      min: 0
      note: "Currently ignored by OpenAI"
    - name: top_p
      type: Optional[float]
      default: NOT_GIVEN
      range: "0.0 to 1.0"
    - name: max_tokens
      type: Optional[int]
      default: NOT_GIVEN
      min: 1
      note: "Deprecated, use max_completion_tokens"
    - name: max_completion_tokens
      type: Optional[int]
      default: NOT_GIVEN
      min: 1
    - name: service_tier
      type: Optional[str]
      default: NOT_GIVEN
    - name: extra
      type: Optional[Dict[str, Any]]
      default: {}
